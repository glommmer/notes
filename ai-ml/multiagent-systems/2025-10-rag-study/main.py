import os
import re
import json
import tqdm
import logging
import operator
from pathlib import Path
from datetime import datetime
from pydantic import BaseModel, Field
from typing import List, TypedDict, Annotated
from langchain.docstore.document import Document
from langchain.document_loaders import PDFPlumberLoader
from langchain.text_splitter import MarkdownHeaderTextSplitter
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langgraph.graph import StateGraph, START, END


# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
AOAI_ENDPOINT = os.getenv("AOAI_ENDPOINT")
AOAI_API_KEY = os.getenv("AOAI_API_KEY")
AOAI_DEPLOY_GPT4O = os.getenv("AOAI_DEPLOY_GPT4O")
AOAI_DEPLOY_GPT4O_MINI = os.getenv("AOAI_DEPLOY_GPT4O_MINI")
AOAI_DEPLOY_EMBED_3_LARGE = os.getenv("AOAI_DEPLOY_EMBED_3_LARGE")

# ë¡œê±° ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler("langgraph_execution.log", encoding="utf-8"),
        logging.StreamHandler(),
    ],
)

logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)
logging.getLogger("urllib3.connectionpool").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)

logger = logging.getLogger(__name__)


# ===========================
# Enhanced Logging Wrapper
# ===========================

class DetailedLogger:
    """ìƒì„¸ ë¡œê·¸ë¥¼ ê¸°ë¡í•˜ëŠ” í—¬í¼ í´ë˜ìŠ¤"""

    @staticmethod
    def log_node_start(node_name: str, input_data: dict = None):
        """ë…¸ë“œ ì‹œì‘ ë¡œê·¸"""
        logger.info("=" * 80)
        logger.info(f"ğŸš€ NODE START: {node_name}")
        logger.info(f"â° Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        if input_data:
            logger.info(f"ğŸ“¥ Input Keys: {list(input_data.keys())}")
        logger.info("=" * 80)

    @staticmethod
    def log_node_end(node_name: str, output_data: dict = None, duration: float = None):
        """ë…¸ë“œ ì¢…ë£Œ ë¡œê·¸"""
        logger.info(f"âœ… NODE END: {node_name}")
        if duration:
            logger.info(f"â±ï¸  Duration: {duration:.2f}s")
        if output_data:
            logger.info(f"ğŸ“¤ Output Keys: {list(output_data.keys())}")
        logger.info("-" * 80 + "\n")

    @staticmethod
    def log_validation(node_name: str, validation_result: dict):
        """ê²€ì¦ ê²°ê³¼ ë¡œê·¸"""
        logger.info(f"ğŸ” VALIDATION RESULT [{node_name}]:")
        logger.info(
            f"  âœ“ Supported by documents: {validation_result.get('is_supported')}"
        )
        logger.info(f"  âœ“ Useful for question: {validation_result.get('is_useful')}")
        logger.info(
            f"  âœ“ Needs refinement: {validation_result.get('needs_refinement')}"
        )

    @staticmethod
    def log_documents(node_name: str, total: int, filtered: int):
        """ë¬¸ì„œ í•„í„°ë§ ë¡œê·¸"""
        logger.info(f"ğŸ“„ DOCUMENT FILTERING [{node_name}]:")
        logger.info(f"  Total retrieved: {total}")
        logger.info(f"  After filtering: {filtered}")
        logger.info(f"  Filter rate: {(filtered / total * 100):.1f}%")

    @staticmethod
    def log_error(node_name: str, error: Exception):
        """ì—ëŸ¬ ë¡œê·¸"""
        logger.error(f"âŒ ERROR in {node_name}: {str(error)}")
        logger.exception(error)


# ===========================
# Custom Reducer for Dictionary Merge
# ===========================

def merge_dicts(existing: dict, updates: dict) -> dict:
    """ë³‘ë ¬ ë…¸ë“œì—ì„œ ë°˜í™˜ëœ dictionaryë¥¼ ë³‘í•©í•˜ëŠ” reducer"""
    if existing is None:
        existing = {}
    merged = existing.copy()
    merged.update(updates)
    return merged


# ===========================
# 1. ë¬¸ì„œ ì²˜ë¦¬ ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„±
# ===========================

def process_pdf(file_path):
    """PDF íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë²•ë ¹ êµ¬ì¡°ì— ë§ê²Œ ì²­í¬ ë¶„í• """
    loader = PDFPlumberLoader(file_path)
    documents = loader.load()

    file_name = os.path.basename(file_path)

    # í…ìŠ¤íŠ¸ íŒ¨í„´ ë³€í™˜ (ë§ˆí¬ë‹¤ìš´ í—¤ë”ë¡œ)
    for doc in documents:
        doc.metadata["file_name"] = file_name
        doc.page_content = re.sub(r"\n(ì œ\d+í¸)", r"\n# \1", doc.page_content)
        doc.page_content = re.sub(r"\n(ì œ\d+ì¥)", r"\n## \1", doc.page_content)
        doc.page_content = re.sub(r"\n(ì œ\d+ì ˆ)", r"\n### \1", doc.page_content)
        doc.page_content = re.sub(r"\n(ì œ\d+ì¡°)", r"\n#### \1", doc.page_content)

    full_text = "\n\n".join([doc.page_content for doc in documents])

    # ë§ˆí¬ë‹¤ìš´ í—¤ë” ê¸°ë°˜ ë¶„í• 
    headers_to_split_on = [
        ("#", "í¸"),
        ("##", "ì¥"),
        ("###", "ì ˆ"),
        ("####", "ì¡°"),
    ]

    markdown_splitter = MarkdownHeaderTextSplitter(
        headers_to_split_on=headers_to_split_on, strip_headers=False
    )

    md_header_splits = markdown_splitter.split_text(full_text)

    for split in md_header_splits:
        split.metadata["file_name"] = file_name

    return md_header_splits


def process_multiple_pdfs(directory_path):
    """ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  PDF ì²˜ë¦¬"""
    pdf_files = list(Path(directory_path).glob("*.pdf"))
    print(f"ì´ {len(pdf_files)}ê°œì˜ PDF íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

    all_splits = []
    for pdf_file in tqdm.tqdm(pdf_files):
        print(f"\nì²˜ë¦¬ ì¤‘: {pdf_file.name}")
        try:
            splits = process_pdf(pdf_file)
            all_splits.extend(splits)
            print(f"  -> {len(splits)}ê°œì˜ ì²­í¬ ìƒì„±")
        except Exception as e:
            print(f"  -> ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            continue

    print(f"\nì´ {len(all_splits)}ê°œì˜ ì²­í¬ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return all_splits


# ===========================
# 2. Multi-Query Retrieval
# ===========================

class MultiQueryGenerator:
    """ì‚¬ìš©ì ì§ˆë¬¸ì„ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì¬ì‘ì„±"""

    def __init__(self, llm):
        self.llm = llm
        self.prompt = ChatPromptTemplate.from_template(
            """
ë‹¹ì‹ ì€ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ 4ê°€ì§€ ë²„ì „ì˜ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.

ì›ë³¸ ì§ˆë¬¸: {question}

ë‹¤ìŒ 4ê°€ì§€ ì „ëµì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì„ ì¬ì‘ì„±í•˜ì„¸ìš”:
1. í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ ë²„ì „ (ë²•ë¥  ìš©ì–´ ì¤‘ì‹¬)
2. êµ¬ì²´ì  ì¡°í•­ ê²€ìƒ‰ ë²„ì „ (ì œXì¡°, ì œXí¸ ë“±)
3. ì‚¬ë¡€ ê¸°ë°˜ ë²„ì „ (ì‹¤ë¬´ ì ìš© ê´€ì )
4. ì ˆì°¨ì  ê´€ì  ë²„ì „ (ì–´ë–»ê²Œ ì§„í–‰ë˜ëŠ”ê°€)

ê° ì§ˆë¬¸ì€ í•œ ì¤„ë¡œ ì‘ì„±í•˜ê³ , ë²ˆí˜¸ë¥¼ ë¶™ì—¬ì£¼ì„¸ìš”.
"""
        )

    def generate_queries(self, question: str) -> List[str]:
        """ë‹¤ì–‘í•œ ê´€ì ì˜ ì§ˆë¬¸ ìƒì„±"""
        chain = self.prompt | self.llm | StrOutputParser()
        result = chain.invoke({"question": question})

        queries = [
            line.strip()
            for line in result.split("\n")
            if line.strip() and any(char.isdigit() for char in line[:3])
        ]
        return [question] + queries


# ===========================
# 3. Self-RAG ê²€ì¦ ë©”ì»¤ë‹ˆì¦˜
# ===========================

class RelevanceGrader(BaseModel):
    """ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ê´€ë ¨ì„± í‰ê°€"""

    binary_score: str = Field(description="ë¬¸ì„œê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ìˆìœ¼ë©´ 'yes', ì•„ë‹ˆë©´ 'no'")
    confidence: int = Field(description="í™•ì‹ ë„ (1-10)")


class AnswerQuality(BaseModel):
    """ìƒì„±ëœ ë‹µë³€ì˜ í’ˆì§ˆ í‰ê°€"""

    is_supported: str = Field(description="ë‹µë³€ì´ ë¬¸ì„œì— ê·¼ê±°í•˜ë©´ 'yes', ì•„ë‹ˆë©´ 'no'")
    is_useful: str = Field(description="ë‹µë³€ì´ ì§ˆë¬¸ì— ìœ ìš©í•˜ë©´ 'yes', ì•„ë‹ˆë©´ 'no'")
    needs_refinement: str = Field(description="ë‹µë³€ ê°œì„ ì´ í•„ìš”í•˜ë©´ 'yes', ì•„ë‹ˆë©´ 'no'")


class SelfRAGValidator:
    """Self-RAG ê²€ì¦ ì‹œìŠ¤í…œ"""

    def __init__(self, llm):
        self.llm = llm
        self.grader = llm.with_structured_output(RelevanceGrader)
        self.quality_checker = llm.with_structured_output(AnswerQuality)

    def grade_documents(
        self, question: str, documents: List[Document]
    ) -> List[Document]:
        """ë¬¸ì„œ ê´€ë ¨ì„± ê²€ì¦"""
        grade_prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    """ë‹¹ì‹ ì€ ë²•ë¥  ë¬¸ì„œ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ í‰ê°€í•˜ì„¸ìš”.
ë²•ë¥  ìš©ì–´, ì¡°í•­ ë²ˆí˜¸, ì ˆì°¨ì  ë‚´ìš©ì´ ì¼ì¹˜í•˜ë©´ ê´€ë ¨ìˆë‹¤ê³  íŒë‹¨í•˜ì„¸ìš”.""",
                ),
                ("human", "ì§ˆë¬¸: {question}\n\në¬¸ì„œ ë‚´ìš©: {document}"),
            ]
        )

        grader_chain = grade_prompt | self.grader
        filtered_docs = []

        for doc in documents:
            score = grader_chain.invoke(
                {"question": question, "document": doc.page_content}
            )

            if score.binary_score == "yes" and score.confidence >= 6:
                filtered_docs.append(doc)

        return filtered_docs

    def validate_answer(
        self, question: str, answer: str, documents: List[Document]
    ) -> dict:
        """ìƒì„±ëœ ë‹µë³€ ê²€ì¦"""
        context = "\n\n".join([doc.page_content for doc in documents])

        quality_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", "ë‹µë³€ì´ ì œê³µëœ ë¬¸ì„œì— ê·¼ê±°í•˜ê³  ì§ˆë¬¸ì— ìœ ìš©í•œì§€ í‰ê°€í•˜ì„¸ìš”."),
                (
                    "human",
                    """
ì§ˆë¬¸: {question}
ë‹µë³€: {answer}
ì°¸ê³  ë¬¸ì„œ: {context}

ë‹µë³€ì„ í‰ê°€í•˜ì„¸ìš”.
""",
                ),
            ]
        )

        quality_chain = quality_prompt | self.quality_checker
        quality = quality_chain.invoke(
            {"question": question, "answer": answer, "context": context}
        )

        return {
            "is_supported": quality.is_supported,
            "is_useful": quality.is_useful,
            "needs_refinement": quality.needs_refinement,
        }


# ===========================
# 4. Multi-Agent ì‹œìŠ¤í…œ
# ===========================

class LegalAgentState(TypedDict):
    """Agent ìƒíƒœ ì •ì˜"""

    question: str
    law_type: str
    queries: List[str]
    documents: List[Document]
    filtered_documents: List[Document]
    answer: str
    validation_result: dict
    needs_refinement: bool


class LegalAgent:
    """íŠ¹ì • ë²•ë¥  ì „ë¬¸ Agent with detailed logging"""

    def __init__(self, law_type: str, llm, retriever, validator):
        self.law_type = law_type
        self.llm = llm
        self.retriever = retriever
        self.validator = validator
        self.logger = DetailedLogger()

        self.prompts = {
            "ê°œì¸ì •ë³´ë³´í˜¸ë²•": """ë‹¹ì‹ ì€ ê°œì¸ì •ë³´ë³´í˜¸ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ê°œì¸ì •ë³´ ì²˜ë¦¬ìì˜ ì˜ë¬´, ì •ë³´ì£¼ì²´ì˜ ê¶Œë¦¬, ìœ„ë°˜ ì‹œ ì œì¬ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.""",
            "í˜•ë²•": """ë‹¹ì‹ ì€ í˜•ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
êµ¬ì„±ìš”ê±´, ìœ„ë²•ì„±, ì±…ì„, í˜•ë²Œì˜ ì¢…ë¥˜ë¥¼ ëª…í™•íˆ ì„¤ëª…í•˜ì„¸ìš”.""",
            "ë¯¼ë²•": """ë‹¹ì‹ ì€ ë¯¼ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë¶ˆë²•í–‰ìœ„, ì†í•´ë°°ìƒì²­êµ¬ê¶Œ, ê³„ì•½ ê´€ê³„ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.""",
            "í˜•ì‚¬ì†Œì†¡ë²•": """ë‹¹ì‹ ì€ í˜•ì‚¬ì†Œì†¡ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ìˆ˜ì‚¬ ì ˆì°¨, ê³µíŒ ì ˆì°¨, ì¦ê±°ë²•ì¹™ì„ ëª…í™•íˆ ì„¤ëª…í•˜ì„¸ìš”.""",
            "ë¯¼ì‚¬ì†Œì†¡ë²•": """ë‹¹ì‹ ì€ ë¯¼ì‚¬ì†Œì†¡ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì†Œì†¡ ì œê¸°, ì‹¬ë¦¬ ì ˆì°¨, íŒê²°ì˜ íš¨ë ¥ì„ ì„¤ëª…í•˜ì„¸ìš”.""",
        }

    def retrieve_documents(self, state: LegalAgentState) -> LegalAgentState:
        """ë‹¤ì¤‘ ì¿¼ë¦¬ë¡œ ë¬¸ì„œ ê²€ìƒ‰ with logging"""
        import time

        start_time = time.time()

        self.logger.log_node_start(
            f"{self.law_type} - Retrieve", {"queries_count": len(state["queries"])}
        )

        try:
            all_docs = []
            for i, query in enumerate(state["queries"], 1):
                logger.info(f"  Query {i}/{len(state['queries'])}: {query[:50]}...")
                docs = self.retriever.invoke(query)
                all_docs.extend(docs)
                logger.info(f"    Retrieved: {len(docs)} documents")

            # ì¤‘ë³µ ì œê±°
            unique_docs = []
            seen_contents = set()
            for doc in all_docs:
                if doc.page_content not in seen_contents:
                    unique_docs.append(doc)
                    seen_contents.add(doc.page_content)

            state["documents"] = unique_docs

            duration = time.time() - start_time
            logger.info(f"ğŸ“Š Total documents: {len(all_docs)}")
            logger.info(f"ğŸ“Š Unique documents: {len(unique_docs)}")
            logger.info(
                f"ğŸ“Š Deduplication rate: {(1 - len(unique_docs) / len(all_docs)) * 100:.1f}%"
            )

            self.logger.log_node_end(
                f"{self.law_type} - Retrieve",
                {"documents_count": len(unique_docs)},
                duration,
            )

        except Exception as e:
            self.logger.log_error(f"{self.law_type} - Retrieve", e)
            raise

        return state

    def grade_documents(self, state: LegalAgentState) -> LegalAgentState:
        """ë¬¸ì„œ ê´€ë ¨ì„± ê²€ì¦ with logging"""
        import time

        start_time = time.time()

        self.logger.log_node_start(
            f"{self.law_type} - Grade", {"documents_to_grade": len(state["documents"])}
        )

        try:
            filtered = self.validator.grade_documents(
                state["question"], state["documents"]
            )

            state["filtered_documents"] = filtered

            duration = time.time() - start_time
            self.logger.log_documents(
                self.law_type, len(state["documents"]), len(filtered)
            )

            # í•„í„°ë§ëœ ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„° ë¡œê·¸
            if filtered:
                logger.info(f"  ğŸ“‘ Sample filtered documents:")
                for i, doc in enumerate(filtered[:3], 1):
                    logger.info(
                        f"    {i}. {doc.metadata.get('file_name', 'Unknown')} - {len(doc.page_content)} chars"
                    )

            self.logger.log_node_end(
                f"{self.law_type} - Grade", {"filtered_count": len(filtered)}, duration
            )

        except Exception as e:
            self.logger.log_error(f"{self.law_type} - Grade", e)
            raise

        return state

    def generate_answer(self, state: LegalAgentState) -> LegalAgentState:
        """ë‹µë³€ ìƒì„± with logging"""
        import time

        start_time = time.time()

        self.logger.log_node_start(
            f"{self.law_type} - Generate",
            {"context_docs": len(state["filtered_documents"])},
        )

        try:
            context = "\n\n".join(
                [doc.page_content for doc in state["filtered_documents"]]
            )
            context_length = len(context)

            logger.info(f"  ğŸ“ Context length: {context_length} characters")
            logger.info(f"  ğŸ“ Question: {state['question'][:100]}...")

            prompt = ChatPromptTemplate.from_template(
                f"""
{self.prompts[self.law_type]}

# ì°¸ê³  ì¡°ë¬¸:
{{context}}

# ì§ˆë¬¸:
{{question}}

# ë‹µë³€ ì‘ì„± ì§€ì¹¨:
1. ê´€ë ¨ ë²•ë¥  ì¡°í•­ì„ ëª…ì‹œí•˜ì„¸ìš” (ì˜ˆ: ê°œì¸ì •ë³´ë³´í˜¸ë²• ì œXì¡°)
2. êµ¬ì²´ì  ì‚¬ë¡€ì— ì ìš©í•˜ì—¬ ì„¤ëª…í•˜ì„¸ìš”
3. ë²•ì  ì±…ì„ê³¼ ì ˆì°¨ë¥¼ ëª…í™•íˆ í•˜ì„¸ìš”
4. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”

# {self.law_type} ê´€ì  ë‹µë³€:
"""
            )

            chain = prompt | self.llm | StrOutputParser()
            answer = chain.invoke({"context": context, "question": state["question"]})

            state["answer"] = answer

            duration = time.time() - start_time
            logger.info(f"  ğŸ“ Answer length: {len(answer)} characters")
            logger.info(f"  ğŸ“ Answer preview: {answer[:200]}...")

            self.logger.log_node_end(
                f"{self.law_type} - Generate", {"answer_length": len(answer)}, duration
            )

        except Exception as e:
            self.logger.log_error(f"{self.law_type} - Generate", e)
            raise

        return state

    def validate_answer(self, state: LegalAgentState) -> LegalAgentState:
        """ë‹µë³€ ê²€ì¦ with logging"""
        import time

        start_time = time.time()

        self.logger.log_node_start(f"{self.law_type} - Validate")

        try:
            validation = self.validator.validate_answer(
                state["question"], state["answer"], state["filtered_documents"]
            )

            state["validation_result"] = validation
            state["needs_refinement"] = validation["needs_refinement"] == "yes"

            duration = time.time() - start_time
            self.logger.log_validation(self.law_type, validation)

            if state["needs_refinement"]:
                logger.warning(f"  âš ï¸  Answer needs refinement!")
            else:
                logger.info(f"  âœ… Answer quality is acceptable")

            self.logger.log_node_end(
                f"{self.law_type} - Validate",
                {"needs_refinement": state["needs_refinement"]},
                duration,
            )

        except Exception as e:
            self.logger.log_error(f"{self.law_type} - Validate", e)
            raise

        return state

    def refine_answer(self, state: LegalAgentState) -> LegalAgentState:
        """ë‹µë³€ ê°œì„  with logging"""
        if not state["needs_refinement"]:
            logger.info(
                f"  â­ï¸  [{self.law_type}] Refinement skipped - answer is acceptable"
            )
            return state

        import time

        start_time = time.time()

        self.logger.log_node_start(f"{self.law_type} - Refine")

        try:
            logger.info(f"  ğŸ”§ Refining answer...")
            logger.info(f"  ğŸ“ Original answer length: {len(state['answer'])} chars")

            refine_prompt = ChatPromptTemplate.from_template(
                """
ì´ì „ ë‹µë³€ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ì‚¬í•­ì„ ê°œì„ í•˜ì—¬ ë‹µë³€ì„ ì¬ì‘ì„±í•˜ì„¸ìš”:

1. ë¬¸ì„œì— ê·¼ê±°í•œ ë‚´ìš©ë§Œ í¬í•¨
2. ë” êµ¬ì²´ì ì¸ ë²•ë¥  ì¡°í•­ ì¸ìš©
3. ì‚¬ë¡€ì— ì§ì ‘ ì ìš© ê°€ëŠ¥í•œ ì„¤ëª…

ì›ë³¸ ì§ˆë¬¸: {question}
ì´ì „ ë‹µë³€: {previous_answer}
ì°¸ê³  ë¬¸ì„œ: {context}

ê°œì„ ëœ ë‹µë³€:
"""
            )

            context = "\n\n".join(
                [doc.page_content for doc in state["filtered_documents"]]
            )
            chain = refine_prompt | self.llm | StrOutputParser()

            improved_answer = chain.invoke(
                {
                    "question": state["question"],
                    "previous_answer": state["answer"],
                    "context": context,
                }
            )

            logger.info(f"  ğŸ“ Refined answer length: {len(improved_answer)} chars")
            logger.info(
                f"  ğŸ“ Length change: {len(improved_answer) - len(state['answer']):+d} chars"
            )

            state["answer"] = improved_answer
            state["needs_refinement"] = False

            duration = time.time() - start_time
            self.logger.log_node_end(
                f"{self.law_type} - Refine",
                {"refined_length": len(improved_answer)},
                duration,
            )

        except Exception as e:
            self.logger.log_error(f"{self.law_type} - Refine", e)
            raise

        return state


# ===========================
# 5. Multi-Agent Supervisor
# ===========================

class SupervisorState(TypedDict):
    """Supervisor ìƒíƒœ - Annotatedë¡œ ë³‘ë ¬ ì—…ë°ì´íŠ¸ ì§€ì›"""

    original_question: str
    sub_questions: Annotated[dict, merge_dicts]  # ë³‘ë ¬ ì—…ë°ì´íŠ¸ í—ˆìš©
    agent_results: Annotated[dict, merge_dicts]  # ë³‘ë ¬ ì—…ë°ì´íŠ¸ í—ˆìš©
    final_answer: str


def create_multi_agent_system(vectorstore):
    """Multi-Agent RAG ì‹œìŠ¤í…œ ìƒì„±"""

    llm = AzureChatOpenAI(
        openai_api_version="2024-08-01-preview",
        azure_deployment=AOAI_DEPLOY_GPT4O_MINI,
        temperature=0.0,
        api_key=AOAI_API_KEY,
        azure_endpoint=AOAI_ENDPOINT,
    )

    retriever = vectorstore.as_retriever(search_kwargs={"k": 10})
    validator = SelfRAGValidator(llm)
    query_generator = MultiQueryGenerator(llm)

    # ë²•ë¥ ë³„ Agent ìƒì„± - ë§¤í•‘ ì‚¬ìš©
    law_types = ["ê°œì¸ì •ë³´ë³´í˜¸ë²•", "í˜•ë²•", "ë¯¼ë²•", "í˜•ì‚¬ì†Œì†¡ë²•", "ë¯¼ì‚¬ì†Œì†¡ë²•"]

    # ì˜ë¬¸ ë…¸ë“œ ì´ë¦„ ë§¤í•‘
    node_name_map = {
        "ê°œì¸ì •ë³´ë³´í˜¸ë²•": "privacy_law",
        "í˜•ë²•": "criminal_law",
        "ë¯¼ë²•": "civil_law",
        "í˜•ì‚¬ì†Œì†¡ë²•": "criminal_procedure",
        "ë¯¼ì‚¬ì†Œì†¡ë²•": "civil_procedure",
    }

    # ì—­ë°©í–¥ ë§¤í•‘
    law_type_map = {v: k for k, v in node_name_map.items()}

    agents = {
        node_name: LegalAgent(law_type, llm, retriever, validator)
        for law_type, node_name in node_name_map.items()
    }

    def supervisor_node(state: SupervisorState) -> SupervisorState:
        """ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ê° ë²•ë¥ ë³„ sub-question ìƒì„±"""
        print("\n=== Supervisor: ì§ˆë¬¸ ë¶„ì„ ì¤‘ ===")

        prompt = ChatPromptTemplate.from_template(
            """
ë‹¤ìŒ ë²•ë¥  ì‚¬ë¡€ë¥¼ ë¶„ì„í•˜ì—¬ ê° ë²•ë¥  ê´€ì ì—ì„œ ë‹µë³€í•´ì•¼ í•  ì„¸ë¶€ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.

ì›ë³¸ ì§ˆë¬¸:
{question}

5ê°€ì§€ ë²•ë¥  ê´€ì :
1. ê°œì¸ì •ë³´ë³´í˜¸ë²•
2. í˜•ë²•
3. ë¯¼ë²•
4. í˜•ì‚¬ì†Œì†¡ë²•
5. ë¯¼ì‚¬ì†Œì†¡ë²•

ê° ë²•ë¥  ê´€ì ì—ì„œ êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•  ì§ˆë¬¸ì„ í•œ ì¤„ì”© ì‘ì„±í•˜ì„¸ìš”.
í˜•ì‹: 
ê°œì¸ì •ë³´ë³´í˜¸ë²•: [ì§ˆë¬¸]
í˜•ë²•: [ì§ˆë¬¸]
...
"""
        )

        chain = prompt | llm | StrOutputParser()
        result = chain.invoke({"question": state["original_question"]})

        # ê²°ê³¼ íŒŒì‹±
        sub_questions = {}
        for line in result.split("\n"):
            line = line.strip()
            if ":" in line:
                law_type = line.split(":")[0].strip()
                question = line.split(":", 1)[1].strip()
                if law_type in law_types:
                    # ì˜ë¬¸ ë…¸ë“œ ì´ë¦„ìœ¼ë¡œ ì €ì¥
                    node_name = node_name_map[law_type]
                    sub_questions[node_name] = question

        return {"sub_questions": sub_questions}

    def agent_executor_node(node_name: str, state: SupervisorState) -> dict:
        """íŠ¹ì • ë²•ë¥  Agent ì‹¤í–‰ - dictionaryë§Œ ë°˜í™˜"""
        agent = agents[node_name]
        question = state["sub_questions"].get(node_name, state["original_question"])

        # Multi-Query ìƒì„±
        queries = query_generator.generate_queries(question)

        # Agent ìƒíƒœ ì´ˆê¸°í™”
        agent_state = LegalAgentState(
            question=question,
            law_type=agent.law_type,  # ì‹¤ì œ í•œê¸€ ë²•ë¥ ëª…
            queries=queries,
            documents=[],
            filtered_documents=[],
            answer="",
            validation_result={},
            needs_refinement=False,
        )

        # Agent ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        agent_state = agent.retrieve_documents(agent_state)
        agent_state = agent.grade_documents(agent_state)
        agent_state = agent.generate_answer(agent_state)
        agent_state = agent.validate_answer(agent_state)
        agent_state = agent.refine_answer(agent_state)

        # agent_resultsì— ì˜ë¬¸ ë…¸ë“œ ì´ë¦„ìœ¼ë¡œ ì €ì¥
        return {"agent_results": {node_name: agent_state["answer"]}}

    def aggregator_node(state: SupervisorState) -> dict:
        """ê° Agent ê²°ê³¼ë¥¼ ì¢…í•©"""
        print("\n=== Aggregator: ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘ ===")

        # ì˜ë¬¸ ë…¸ë“œ ì´ë¦„ì„ í•œê¸€ë¡œ ë³€í™˜í•˜ì—¬ ë‹µë³€ ê²°í•©
        combined_answers = "\n\n".join(
            [
                f"## {law_type_map[node_name]}\n{answer}"
                for node_name, answer in state["agent_results"].items()
            ]
        )

        final_prompt = ChatPromptTemplate.from_template(
            """
ë‹¤ìŒì€ 5ê°€ì§€ ë²•ë¥  ê´€ì ì—ì„œ ë¶„ì„í•œ ê²°ê³¼ì…ë‹ˆë‹¤.
ì´ë¥¼ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ êµ¬ì¡°í™”ëœ ìµœì¢… ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.

ì›ë³¸ ì§ˆë¬¸:
{question}

ê° ë²•ë¥ ë³„ ë¶„ì„:
{combined_answers}

ìµœì¢… ì¢…í•© ë‹µë³€ (ê° ë²•ë¥  ê´€ì ì„ ëª¨ë‘ í¬í•¨í•˜ì—¬ êµ¬ì¡°í™”):
        """
        )

        chain = final_prompt | llm | StrOutputParser()
        final_answer = chain.invoke(
            {
                "question": state["original_question"],
                "combined_answers": combined_answers,
            }
        )

        return {"final_answer": final_answer}

    # LangGraph êµ¬ì„±
    workflow = StateGraph(SupervisorState)

    # ë…¸ë“œ ì¶”ê°€ - ì˜ë¬¸ ì´ë¦„ ì‚¬ìš©
    workflow.add_node("supervisor", supervisor_node)
    for node_name in node_name_map.values():
        workflow.add_node(node_name, lambda s, nn=node_name: agent_executor_node(nn, s))
    workflow.add_node("aggregator", aggregator_node)

    # ì—£ì§€ êµ¬ì„±
    workflow.add_edge(START, "supervisor")
    for node_name in node_name_map.values():
        workflow.add_edge("supervisor", node_name)
        workflow.add_edge(node_name, "aggregator")
    workflow.add_edge("aggregator", END)

    return workflow.compile()


# ===========================
# 6. ì‹¤í–‰ ì½”ë“œ
# ===========================

def main():
    # 1. PDF ì²˜ë¦¬ ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„± (ë˜ëŠ” ê¸°ì¡´ ì €ì¥ì†Œ ë¡œë“œ)
    print("=== ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ===")

    embeddings = AzureOpenAIEmbeddings(
        model=AOAI_DEPLOY_EMBED_3_LARGE,
        openai_api_version="2024-08-01-preview",
        api_key=AOAI_API_KEY,
        azure_endpoint=AOAI_ENDPOINT,
    )

    # ê¸°ì¡´ ì €ì¥ì†Œê°€ ìˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
    try:
        vectorstore = FAISS.load_local(
            folder_path="./faiss_db2",
            embeddings=embeddings,
            allow_dangerous_deserialization=True,
        )
        print("ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    except:
        print("=== PDF ì²˜ë¦¬ ì‹œì‘ ===")
        directory = "./pdf/"
        all_chunks = process_multiple_pdfs(directory)

        print("\n=== ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘ ===")
        vectorstore = FAISS.from_documents(
            documents=all_chunks,
            embedding=embeddings,
        )

        vectorstore.save_local(folder_path="./faiss_db2")

    # 2. Multi-Agent ì‹œìŠ¤í…œ ìƒì„±
    print("\n=== Multi-Agent ì‹œìŠ¤í…œ ì´ˆê¸°í™” ===")
    app = create_multi_agent_system(vectorstore)

    # from IPython.display import Image, display
    #
    # display(
    #     Image(
    #         app.get_graph().draw_mermaid_png()
    #     )
    # )

    # 3. ì§ˆë¬¸ ì‹¤í–‰
    question = """
[ì‚¬ë¡€]
ì˜¨ë¼ì¸ ì‡¼í•‘ëª° 'A'ì‚¬ì˜ ê°œë°œíŒ€ì—ì„œ ê·¼ë¬´í•˜ë˜ **ê°‘(ç”²)**ì€ í‡´ì‚¬ ì§ì „, 
íšŒì‚¬ì˜ ê³ ê° ê´€ë¦¬ ë°ì´í„°ë² ì´ìŠ¤ì— ì ‘ê·¼í•˜ì—¬ ê³ ê° 1ë§Œ ëª…ì˜ ì´ë¦„, ì£¼ì†Œ, ì—°ë½ì²˜, êµ¬ë§¤ ë‚´ì—­ ë“± ê°œì¸ì •ë³´ë¥¼ 
ìì‹ ì˜ ê°œì¸ ì„œë²„ë¡œ ë¬´ë‹¨ ìœ ì¶œí–ˆìŠµë‹ˆë‹¤.

ì´í›„ **ê°‘(ç”²)**ì€ ìœ ì¶œí•œ ê°œì¸ì •ë³´ ì¤‘ ì¼ë¶€ë¥¼ ì´ìš©í•˜ì—¬ 
íŠ¹ì • ê³ ê° **ì„(ä¹™)**ì—ê²Œ ì „í™”ë¥¼ ê±¸ì–´ 'A'ì‚¬ ì§ì›ì„ ì‚¬ì¹­í•˜ë©° "ì‹œìŠ¤í…œ ì˜¤ë¥˜ë¡œ ê²°ì œê°€ ì¤‘ë³µ ì²˜ë¦¬ë˜ì—ˆìœ¼ë‹ˆ, 
í™˜ë¶ˆì„ ìœ„í•´ ì•Œë ¤ì£¼ëŠ” íŠ¹ì • ê³„ì¢Œë¡œ ìˆ˜ìˆ˜ë£Œ 10ë§Œ ì›ì„ ë¨¼ì € ì…ê¸ˆí•˜ë¼"ê³  ì†ì—¬ ëˆì„ í¸ì·¨í–ˆìŠµë‹ˆë‹¤.

ì´ ì‚¬ì‹¤ì„ ë’¤ëŠ¦ê²Œ ì•Œê²Œ ëœ 'A'ì‚¬ëŠ” ë‚´ë¶€ ê°ì‚¬ë¥¼ í†µí•´ **ê°‘(ç”²)**ì˜ ì†Œí–‰ì„ì„ íŒŒì•…í–ˆê³ , 
ê³ ê° **ì„(ä¹™)**ì„ í¬í•¨í•œ ë‹¤ìˆ˜ì˜ í”¼í•´ìëŠ” 'A'ì‚¬ë¥¼ ìƒëŒ€ë¡œ ì§‘ë‹¨ì ìœ¼ë¡œ í•­ì˜í•˜ê¸° ì‹œì‘í–ˆìŠµë‹ˆë‹¤.

[ë¬¸ì œ]
ìœ„ ì‚¬ë¡€ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ê°‘(ç”²), 'A'ì‚¬, ê·¸ë¦¬ê³  í”¼í•´ ê³ ê° ì„(ä¹™) ì‚¬ì´ì— ë°œìƒí•  ìˆ˜ ìˆëŠ” ë²•ì  ë¬¸ì œë“¤ì„ 
ì•„ë˜ 5ê°€ì§€ ë²•ë¥ ì˜ ê´€ì ì—ì„œ ê°ê° ë¶„ì„í•˜ê³  ê·¸ ê·¼ê±° ì¡°í•­ê³¼ í•¨ê»˜ ì„¤ëª…í•˜ì‹œì˜¤.

1. ê°œì¸ì •ë³´ë³´í˜¸ë²•: 'A'ì‚¬ì™€ ê°‘(ç”²)ì€ ê°ê° ì–´ë–¤ ì˜ë¬´ë¥¼ ìœ„ë°˜í–ˆìœ¼ë©°, ì–´ë–¤ ì±…ì„ì„ ì§€ê²Œ ë˜ëŠ”ê°€?
2. í˜•ë²•: ê°‘(ç”²)ì˜ í–‰ìœ„ëŠ” ì–´ë–¤ ë²”ì£„ì— í•´ë‹¹í•˜ë©°, ê·¸ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€?
3. ë¯¼ë²•: ì„(ä¹™)ì€ ê°‘(ç”²)ê³¼ 'A'ì‚¬ë¥¼ ìƒëŒ€ë¡œ ì–´ë–¤ ê¶Œë¦¬ë¥¼ ì£¼ì¥í•˜ë©° ì†í•´ë°°ìƒì„ ì²­êµ¬í•  ìˆ˜ ìˆëŠ”ê°€? ê·¸ ë²•ì  ê·¼ê±°ëŠ” ë¬´ì—‡ì¸ê°€?
4. í˜•ì‚¬ì†Œì†¡ë²•: ê°‘(ç”²)ì˜ ë²”ì£„ í˜ì˜ì— ëŒ€í•´ ìˆ˜ì‚¬ê¸°ê´€ì´ ìˆ˜ì‚¬ë¥¼ ê°œì‹œí•˜ê³  ì¬íŒì— ë„˜ê¸°ëŠ” ê³¼ì •ì€ ì–´ë–»ê²Œ ì§„í–‰ë˜ëŠ”ê°€?
5. ë¯¼ì‚¬ì†Œì†¡ë²•: ì„(ä¹™)ì´ ìì‹ ì˜ ê¸ˆì „ì , ì •ì‹ ì  í”¼í•´ë¥¼ êµ¬ì œë°›ê¸° ìœ„í•´ ë²•ì›ì— ì†Œì†¡ì„ ì œê¸°í•œë‹¤ë©´, ê·¸ ì ˆì°¨ëŠ” ì–´ë–»ê²Œ ì§„í–‰ë˜ëŠ”ê°€?
"""

    print("\n=== ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘ ===")
    logger.info("=" * 80)
    logger.info("ğŸ¬ EXECUTION START")
    logger.info("=" * 80)

    state = {
        "original_question": question,
        "sub_questions": {},
        "agent_results": {},
        "final_answer": "",
    }

    # final_state = None
    # for update in app.stream(state, stream_mode="updates"):
    #     logger.info(f"ğŸ“¦ State Update: {list(update.keys())}")
    #     for node_name, node_output in update.items():
    #         logger.info(f"  [{node_name}] completed")
    #         # ê° ë…¸ë“œì˜ ì¶œë ¥ ìƒì„¸ ë¡œê·¸
    #         if isinstance(node_output, dict):
    #             for key, value in node_output.items():
    #                 if isinstance(value, str):
    #                     logger.info(f"    {key}: {value[:100]}...")
    #                 else:
    #                     logger.info(f"    {key}: {type(value)}")
    #     final_state = update

    result = app.invoke(state)

    logger.info("=" * 80)
    logger.info("ğŸ EXECUTION COMPLETE")
    logger.info("=" * 80)

    print("\n" + "=" * 80)
    print("ìµœì¢… ë‹µë³€:")
    print("=" * 80)
    print(result["final_answer"])


if __name__ == "__main__":
    main()
