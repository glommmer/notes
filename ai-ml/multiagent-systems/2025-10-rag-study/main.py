import os
import re
import json
import tqdm
import logging
import operator
from pathlib import Path
from datetime import datetime
from pydantic import BaseModel, Field
from typing import List, TypedDict, Annotated
from langchain.docstore.document import Document
from langchain.document_loaders import PDFPlumberLoader
from langchain.text_splitter import MarkdownHeaderTextSplitter
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langgraph.graph import StateGraph, START, END


# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
AOAI_ENDPOINT = os.getenv("AOAI_ENDPOINT")
AOAI_API_KEY = os.getenv("AOAI_API_KEY")
AOAI_DEPLOY_GPT4O = os.getenv("AOAI_DEPLOY_GPT4O")
AOAI_DEPLOY_GPT4O_MINI = os.getenv("AOAI_DEPLOY_GPT4O_MINI")
AOAI_DEPLOY_EMBED_3_LARGE = os.getenv("AOAI_DEPLOY_EMBED_3_LARGE")

# ë¡œê±° ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler("langgraph_execution.log", encoding="utf-8"),
        logging.StreamHandler(),
    ],
)

logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)
logging.getLogger("urllib3.connectionpool").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)

logger = logging.getLogger(__name__)


# ===========================
# Enhanced Logging Wrapper
# ===========================

class DetailedLogger:
    """ìƒì„¸ ë¡œê·¸ë¥¼ ê¸°ë¡í•˜ëŠ” í—¬í¼ í´ë˜ìŠ¤"""

    @staticmethod
    def log_node_start(node_name: str, input_data: dict = None):
        """ë…¸ë“œ ì‹œì‘ ë¡œê·¸"""
        logger.info("=" * 80)
        logger.info(f"ğŸš€ NODE START: {node_name}")
        logger.info(f"â° Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        if input_data:
            logger.info(f"ğŸ“¥ Input Keys: {list(input_data.keys())}")
        logger.info("=" * 80)

    @staticmethod
    def log_node_end(node_name: str, output_data: dict = None, duration: float = None):
        """ë…¸ë“œ ì¢…ë£Œ ë¡œê·¸"""
        logger.info(f"âœ… NODE END: {node_name}")
        if duration:
            logger.info(f"â±ï¸  Duration: {duration:.2f}s")
        if output_data:
            logger.info(f"ğŸ“¤ Output Keys: {list(output_data.keys())}")
        logger.info("-" * 80 + "\n")

    @staticmethod
    def log_validation(node_name: str, validation_result: dict):
        """ê²€ì¦ ê²°ê³¼ ë¡œê·¸"""
        logger.info(f"ğŸ” VALIDATION RESULT [{node_name}]:")
        logger.info(
            f"  âœ“ Supported by documents: {validation_result.get('is_supported')}"
        )
        logger.info(f"  âœ“ Useful for question: {validation_result.get('is_useful')}")
        logger.info(
            f"  âœ“ Needs refinement: {validation_result.get('needs_refinement')}"
        )

    @staticmethod
    def log_documents(node_name: str, total: int, filtered: int):
        """ë¬¸ì„œ í•„í„°ë§ ë¡œê·¸"""
        logger.info(f"ğŸ“„ DOCUMENT FILTERING [{node_name}]:")
        logger.info(f"  Total retrieved: {total}")
        logger.info(f"  After filtering: {filtered}")
        logger.info(f"  Filter rate: {(filtered / total * 100):.1f}%")

    @staticmethod
    def log_error(node_name: str, error: Exception):
        """ì—ëŸ¬ ë¡œê·¸"""
        logger.error(f"âŒ ERROR in {node_name}: {str(error)}")
        logger.exception(error)


# ===========================
# 1. ë¬¸ì„œ ì²˜ë¦¬ ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„±
# ===========================

def process_pdf(file_path):
    """PDF íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë²•ë ¹ êµ¬ì¡°ì— ë§ê²Œ ì²­í¬ ë¶„í• """
    loader = PDFPlumberLoader(file_path)
    documents = loader.load()

    file_name = os.path.basename(file_path)

    # í…ìŠ¤íŠ¸ íŒ¨í„´ ë³€í™˜ (ë§ˆí¬ë‹¤ìš´ í—¤ë”ë¡œ)
    for doc in documents:
        doc.metadata["file_name"] = file_name
        doc.page_content = re.sub(r"\n(ì œ\d+í¸)", r"\n# \1", doc.page_content)
        doc.page_content = re.sub(r"\n(ì œ\d+ì¥)", r"\n## \1", doc.page_content)
        doc.page_content = re.sub(r"\n(ì œ\d+ì ˆ)", r"\n### \1", doc.page_content)
        doc.page_content = re.sub(r"\n(ì œ\d+ì¡°)", r"\n#### \1", doc.page_content)

    full_text = "\n\n".join([doc.page_content for doc in documents])

    # ë§ˆí¬ë‹¤ìš´ í—¤ë” ê¸°ë°˜ ë¶„í• 
    headers_to_split_on = [
        ("#", "í¸"),
        ("##", "ì¥"),
        ("###", "ì ˆ"),
        ("####", "ì¡°"),
    ]

    markdown_splitter = MarkdownHeaderTextSplitter(
        headers_to_split_on=headers_to_split_on, strip_headers=False
    )

    md_header_splits = markdown_splitter.split_text(full_text)

    for split in md_header_splits:
        split.metadata["file_name"] = file_name

    return md_header_splits


def process_multiple_pdfs(directory_path):
    """ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  PDF ì²˜ë¦¬"""
    pdf_files = list(Path(directory_path).glob("*.pdf"))
    print(f"ì´ {len(pdf_files)}ê°œì˜ PDF íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

    all_splits = []
    for pdf_file in tqdm.tqdm(pdf_files):
        print(f"\nì²˜ë¦¬ ì¤‘: {pdf_file.name}")
        try:
            splits = process_pdf(pdf_file)
            all_splits.extend(splits)
            print(f"  -> {len(splits)}ê°œì˜ ì²­í¬ ìƒì„±")
        except Exception as e:
            print(f"  -> ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            continue

    print(f"\nì´ {len(all_splits)}ê°œì˜ ì²­í¬ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return all_splits


# ===========================
# 2. Multi-Query Retrieval
# ===========================

class MultiQueryGenerator:
    """ì‚¬ìš©ì ì§ˆë¬¸ì„ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì¬ì‘ì„±"""

    def __init__(self, llm):
        self.llm = llm
        self.prompt = ChatPromptTemplate.from_template(
            """
ë‹¹ì‹ ì€ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ 4ê°€ì§€ ë²„ì „ì˜ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.

ì›ë³¸ ì§ˆë¬¸: {question}

ë‹¤ìŒ 4ê°€ì§€ ì „ëµì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì„ ì¬ì‘ì„±í•˜ì„¸ìš”:
1. í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ ë²„ì „ (ë²•ë¥  ìš©ì–´ ì¤‘ì‹¬)
2. êµ¬ì²´ì  ì¡°í•­ ê²€ìƒ‰ ë²„ì „ (ì œXì¡°, ì œXí¸ ë“±)
3. ì‚¬ë¡€ ê¸°ë°˜ ë²„ì „ (ì‹¤ë¬´ ì ìš© ê´€ì )
4. ì ˆì°¨ì  ê´€ì  ë²„ì „ (ì–´ë–»ê²Œ ì§„í–‰ë˜ëŠ”ê°€)

ê° ì§ˆë¬¸ì€ í•œ ì¤„ë¡œ ì‘ì„±í•˜ê³ , ë²ˆí˜¸ë¥¼ ë¶™ì—¬ì£¼ì„¸ìš”.
"""
        )

    def generate_queries(self, question: str) -> List[str]:
        """ë‹¤ì–‘í•œ ê´€ì ì˜ ì§ˆë¬¸ ìƒì„±"""
        chain = self.prompt | self.llm | StrOutputParser()
        result = chain.invoke({"question": question})

        queries = [
            line.strip()
            for line in result.split("\n")
            if line.strip() and any(char.isdigit() for char in line[:3])
        ]
        return [question] + queries


# ===========================
# 3. Self-RAG ê²€ì¦ ë©”ì»¤ë‹ˆì¦˜
# ===========================

class RelevanceGrader(BaseModel):
    """ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ê´€ë ¨ì„± í‰ê°€"""

    binary_score: str = Field(description="ë¬¸ì„œê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ìˆìœ¼ë©´ 'yes', ì•„ë‹ˆë©´ 'no'")
    confidence: int = Field(description="í™•ì‹ ë„ (1-10)")


class AnswerQuality(BaseModel):
    """ìƒì„±ëœ ë‹µë³€ì˜ í’ˆì§ˆ í‰ê°€"""

    is_supported: str = Field(description="ë‹µë³€ì´ ë¬¸ì„œì— ê·¼ê±°í•˜ë©´ 'yes', ì•„ë‹ˆë©´ 'no'")
    is_useful: str = Field(description="ë‹µë³€ì´ ì§ˆë¬¸ì— ìœ ìš©í•˜ë©´ 'yes', ì•„ë‹ˆë©´ 'no'")
    needs_refinement: str = Field(description="ë‹µë³€ ê°œì„ ì´ í•„ìš”í•˜ë©´ 'yes', ì•„ë‹ˆë©´ 'no'")


class SelfRAGValidator:
    """Self-RAG ê²€ì¦ ì‹œìŠ¤í…œ"""

    def __init__(self, llm):
        self.llm = llm
        self.grader = llm.with_structured_output(RelevanceGrader)
        self.quality_checker = llm.with_structured_output(AnswerQuality)

    def grade_documents(
        self, question: str, documents: List[Document]
    ) -> List[Document]:
        """ë¬¸ì„œ ê´€ë ¨ì„± ê²€ì¦"""
        grade_prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    """ë‹¹ì‹ ì€ ë²•ë¥  ë¬¸ì„œ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ í‰ê°€í•˜ì„¸ìš”.
ë²•ë¥  ìš©ì–´, ì¡°í•­ ë²ˆí˜¸, ì ˆì°¨ì  ë‚´ìš©ì´ ì¼ì¹˜í•˜ë©´ ê´€ë ¨ìˆë‹¤ê³  íŒë‹¨í•˜ì„¸ìš”.""",
                ),
                ("human", "ì§ˆë¬¸: {question}\n\në¬¸ì„œ ë‚´ìš©: {document}"),
            ]
        )

        grader_chain = grade_prompt | self.grader
        filtered_docs = []

        for doc in documents:
            score = grader_chain.invoke(
                {"question": question, "document": doc.page_content}
            )

            if score.binary_score == "yes" and score.confidence >= 6:
                filtered_docs.append(doc)

        return filtered_docs

    def validate_answer(
        self, question: str, answer: str, documents: List[Document]
    ) -> dict:
        """ìƒì„±ëœ ë‹µë³€ ê²€ì¦"""
        context = "\n\n".join([doc.page_content for doc in documents])

        quality_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", "ë‹µë³€ì´ ì œê³µëœ ë¬¸ì„œì— ê·¼ê±°í•˜ê³  ì§ˆë¬¸ì— ìœ ìš©í•œì§€ í‰ê°€í•˜ì„¸ìš”."),
                (
                    "human",
                    """
ì§ˆë¬¸: {question}
ë‹µë³€: {answer}
ì°¸ê³  ë¬¸ì„œ: {context}

ë‹µë³€ì„ í‰ê°€í•˜ì„¸ìš”.
""",
                ),
            ]
        )

        quality_chain = quality_prompt | self.quality_checker
        quality = quality_chain.invoke(
            {"question": question, "answer": answer, "context": context}
        )

        return {
            "is_supported": quality.is_supported,
            "is_useful": quality.is_useful,
            "needs_refinement": quality.needs_refinement,
        }


def main():
    # 1. PDF ì²˜ë¦¬ ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„± (ë˜ëŠ” ê¸°ì¡´ ì €ì¥ì†Œ ë¡œë“œ)
    print("=== ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ===")

    embeddings = AzureOpenAIEmbeddings(
        model=AOAI_DEPLOY_EMBED_3_LARGE,
        openai_api_version="2024-08-01-preview",
        api_key=AOAI_API_KEY,
        azure_endpoint=AOAI_ENDPOINT,
    )

    # ê¸°ì¡´ ì €ì¥ì†Œê°€ ìˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
    try:
        vectorstore = FAISS.load_local(
            folder_path="./faiss_db2",
            embeddings=embeddings,
            allow_dangerous_deserialization=True,
        )
        print("ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    except:
        print("=== PDF ì²˜ë¦¬ ì‹œì‘ ===")
        directory = "./pdf/"
        all_chunks = process_multiple_pdfs(directory)

        print("\n=== ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘ ===")
        vectorstore = FAISS.from_documents(
            documents=all_chunks,
            embedding=embeddings,
        )

        vectorstore.save_local(folder_path="./faiss_db2")

    print("\n[Step 2] LLM ì´ˆê¸°í™”")
    print("-" * 80)

    llm = AzureChatOpenAI(
        openai_api_version="2024-08-01-preview",
        azure_deployment=AOAI_DEPLOY_GPT4O_MINI,
        temperature=0.0,
        api_key=AOAI_API_KEY,
        azure_endpoint=AOAI_ENDPOINT,
    )
    print("âœ… Azure OpenAI LLM ì´ˆê¸°í™” ì™„ë£Œ")

    print("\n[Step 3] Retriever í…ŒìŠ¤íŠ¸")
    print("-" * 80)

    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

    test_query = "ê°œì¸ì •ë³´ ìœ ì¶œ ì‹œ ì²˜ë¦¬ìì˜ ì±…ì„ì€?"
    print(f"í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: {test_query}")

    docs = retriever.invoke(test_query)
    print(f"âœ… ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(docs)}")

    if docs:
        print(f"\nì²« ë²ˆì§¸ ë¬¸ì„œ ìƒ˜í”Œ:")
        print(f"  - íŒŒì¼ëª…: {docs[0].metadata.get('file_name', 'Unknown')}")
        print(f"  - ë‚´ìš© ê¸¸ì´: {len(docs[0].page_content)} ë¬¸ì")
        print(f"  - ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {docs[0].page_content[:200]}...")

    print("\n[Step 4] Multi-Query Generator í…ŒìŠ¤íŠ¸")
    print("-" * 80)

    query_generator = MultiQueryGenerator(llm)

    original_question = "ê°œì¸ì •ë³´ë³´í˜¸ë²• ìœ„ë°˜ ì‹œ ì²˜ë²Œ ì¡°í•­ì€?"
    print(f"ì›ë³¸ ì§ˆë¬¸: {original_question}")

    generated_queries = query_generator.generate_queries(original_question)
    print(f"\nâœ… ìƒì„±ëœ ì¿¼ë¦¬ ìˆ˜: {len(generated_queries)}")

    for i, query in enumerate(generated_queries, 1):
        print(f"  {i}. {query}")

    # Multi-Queryë¡œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print("\nê° ì¿¼ë¦¬ë¡œ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
    all_retrieved_docs = []
    for i, query in enumerate(generated_queries, 1):
        docs = retriever.invoke(query)
        all_retrieved_docs.extend(docs)
        print(f"  ì¿¼ë¦¬ {i}: {len(docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨")

    # ì¤‘ë³µ ì œê±°
    unique_docs = []
    seen_contents = set()
    for doc in all_retrieved_docs:
        if doc.page_content not in seen_contents:
            unique_docs.append(doc)
            seen_contents.add(doc.page_content)

    print(f"\nâœ… ì´ ê²€ìƒ‰: {len(all_retrieved_docs)}ê°œ â†’ ì¤‘ë³µ ì œê±° í›„: {len(unique_docs)}ê°œ")

    print("\n[Step 5] Self-RAG Validator í…ŒìŠ¤íŠ¸")
    print("-" * 80)

    validator = SelfRAGValidator(llm)

    # ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€
    print("\n[5-1] ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ (Document Grading)")
    test_question = "ê°œì¸ì •ë³´ë³´í˜¸ë²•ì—ì„œ ì •ë³´ ìœ ì¶œ ì‹œ ì²˜ë¦¬ìì˜ ì˜ë¬´ëŠ”?"
    print(f"ì§ˆë¬¸: {test_question}")
    print(f"í‰ê°€ ëŒ€ìƒ ë¬¸ì„œ ìˆ˜: {len(unique_docs)}")

    filtered_docs = validator.grade_documents(test_question, unique_docs)
    print(f"\nâœ… ê´€ë ¨ì„± í‰ê°€ ì™„ë£Œ:")
    print(f"  - ì›ë³¸ ë¬¸ì„œ: {len(unique_docs)}ê°œ")
    print(f"  - í•„í„°ë§ í›„: {len(filtered_docs)}ê°œ")
    print(f"  - í•„í„°ë§ ë¹„ìœ¨: {(len(filtered_docs) / len(unique_docs) * 100):.1f}%")

    if filtered_docs:
        print(f"\ní•„í„°ë§ëœ ë¬¸ì„œ ìƒ˜í”Œ:")
        for i, doc in enumerate(filtered_docs[:2], 1):
            print(f"  {i}. {doc.metadata.get('file_name', 'Unknown')}")
            print(f"     ë‚´ìš©: {doc.page_content[:150]}...")

    # ë‹µë³€ ìƒì„± (ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ìš©)
    print("\n[5-2] ë‹µë³€ ìƒì„± ë° í’ˆì§ˆ ê²€ì¦")

    if filtered_docs:
        context = "\n\n".join([doc.page_content for doc in filtered_docs[:3]])

        # ê°„ë‹¨í•œ ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸
        answer_prompt = ChatPromptTemplate.from_template("""
    ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

    ì°¸ê³  ë¬¸ì„œ:
    {context}

    ì§ˆë¬¸: {question}

    ë‹µë³€:
    """)

        chain = answer_prompt | llm | StrOutputParser()
        test_answer = chain.invoke({
            "context": context,
            "question": test_question
        })

        print(f"ìƒì„±ëœ ë‹µë³€ (ê¸¸ì´: {len(test_answer)} ë¬¸ì):")
        print(f"{test_answer[:300]}...")

        # ë‹µë³€ í’ˆì§ˆ ê²€ì¦
        print("\n[5-3] ë‹µë³€ í’ˆì§ˆ ê²€ì¦")
        validation_result = validator.validate_answer(
            test_question,
            test_answer,
            filtered_docs[:3]
        )

        print("\nâœ… ê²€ì¦ ê²°ê³¼:")
        print(f"  - ë¬¸ì„œ ê·¼ê±° ì—¬ë¶€: {validation_result['is_supported']}")
        print(f"  - ìœ ìš©ì„±: {validation_result['is_useful']}")
        print(f"  - ê°œì„  í•„ìš”: {validation_result['needs_refinement']}")

        if validation_result['needs_refinement'] == 'yes':
            print("\nâš ï¸  ë‹µë³€ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        else:
            print("\nâœ… ë‹µë³€ í’ˆì§ˆì´ ì–‘í˜¸í•©ë‹ˆë‹¤.")


if __name__ == "__main__":
    main()

