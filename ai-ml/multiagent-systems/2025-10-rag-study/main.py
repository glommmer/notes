import os
import re
import tqdm
import logging
from pathlib import Path
from datetime import datetime
from langchain.document_loaders import PDFPlumberLoader
from langchain.text_splitter import MarkdownHeaderTextSplitter
from langchain_openai import AzureOpenAIEmbeddings
from langchain_community.vectorstores import FAISS


# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
AOAI_ENDPOINT = os.getenv("AOAI_ENDPOINT")
AOAI_API_KEY = os.getenv("AOAI_API_KEY")
AOAI_DEPLOY_GPT4O = os.getenv("AOAI_DEPLOY_GPT4O")
AOAI_DEPLOY_GPT4O_MINI = os.getenv("AOAI_DEPLOY_GPT4O_MINI")
AOAI_DEPLOY_EMBED_3_LARGE = os.getenv("AOAI_DEPLOY_EMBED_3_LARGE")

# ë¡œê±° ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler("langgraph_execution.log", encoding="utf-8"),
        logging.StreamHandler(),
    ],
)

logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)
logging.getLogger("urllib3.connectionpool").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)

logger = logging.getLogger(__name__)


# ===========================
# Enhanced Logging Wrapper
# ===========================

class DetailedLogger:
    """ìƒì„¸ ë¡œê·¸ë¥¼ ê¸°ë¡í•˜ëŠ” í—¬í¼ í´ë˜ìŠ¤"""

    @staticmethod
    def log_node_start(node_name: str, input_data: dict = None):
        """ë…¸ë“œ ì‹œì‘ ë¡œê·¸"""
        logger.info("=" * 80)
        logger.info(f"ğŸš€ NODE START: {node_name}")
        logger.info(f"â° Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        if input_data:
            logger.info(f"ğŸ“¥ Input Keys: {list(input_data.keys())}")
        logger.info("=" * 80)

    @staticmethod
    def log_node_end(node_name: str, output_data: dict = None, duration: float = None):
        """ë…¸ë“œ ì¢…ë£Œ ë¡œê·¸"""
        logger.info(f"âœ… NODE END: {node_name}")
        if duration:
            logger.info(f"â±ï¸  Duration: {duration:.2f}s")
        if output_data:
            logger.info(f"ğŸ“¤ Output Keys: {list(output_data.keys())}")
        logger.info("-" * 80 + "\n")

    @staticmethod
    def log_validation(node_name: str, validation_result: dict):
        """ê²€ì¦ ê²°ê³¼ ë¡œê·¸"""
        logger.info(f"ğŸ” VALIDATION RESULT [{node_name}]:")
        logger.info(
            f"  âœ“ Supported by documents: {validation_result.get('is_supported')}"
        )
        logger.info(f"  âœ“ Useful for question: {validation_result.get('is_useful')}")
        logger.info(
            f"  âœ“ Needs refinement: {validation_result.get('needs_refinement')}"
        )

    @staticmethod
    def log_documents(node_name: str, total: int, filtered: int):
        """ë¬¸ì„œ í•„í„°ë§ ë¡œê·¸"""
        logger.info(f"ğŸ“„ DOCUMENT FILTERING [{node_name}]:")
        logger.info(f"  Total retrieved: {total}")
        logger.info(f"  After filtering: {filtered}")
        logger.info(f"  Filter rate: {(filtered / total * 100):.1f}%")

    @staticmethod
    def log_error(node_name: str, error: Exception):
        """ì—ëŸ¬ ë¡œê·¸"""
        logger.error(f"âŒ ERROR in {node_name}: {str(error)}")
        logger.exception(error)


# ===========================
# 1. ë¬¸ì„œ ì²˜ë¦¬ ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„±
# ===========================

def process_pdf(file_path):
    """PDF íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë²•ë ¹ êµ¬ì¡°ì— ë§ê²Œ ì²­í¬ ë¶„í• """
    loader = PDFPlumberLoader(file_path)
    documents = loader.load()

    file_name = os.path.basename(file_path)

    # í…ìŠ¤íŠ¸ íŒ¨í„´ ë³€í™˜ (ë§ˆí¬ë‹¤ìš´ í—¤ë”ë¡œ)
    for doc in documents:
        doc.metadata["file_name"] = file_name
        doc.page_content = re.sub(r"\n(ì œ\d+í¸)", r"\n# \1", doc.page_content)
        doc.page_content = re.sub(r"\n(ì œ\d+ì¥)", r"\n## \1", doc.page_content)
        doc.page_content = re.sub(r"\n(ì œ\d+ì ˆ)", r"\n### \1", doc.page_content)
        doc.page_content = re.sub(r"\n(ì œ\d+ì¡°)", r"\n#### \1", doc.page_content)

    full_text = "\n\n".join([doc.page_content for doc in documents])

    # ë§ˆí¬ë‹¤ìš´ í—¤ë” ê¸°ë°˜ ë¶„í• 
    headers_to_split_on = [
        ("#", "í¸"),
        ("##", "ì¥"),
        ("###", "ì ˆ"),
        ("####", "ì¡°"),
    ]

    markdown_splitter = MarkdownHeaderTextSplitter(
        headers_to_split_on=headers_to_split_on, strip_headers=False
    )

    md_header_splits = markdown_splitter.split_text(full_text)

    for split in md_header_splits:
        split.metadata["file_name"] = file_name

    return md_header_splits


def process_multiple_pdfs(directory_path):
    """ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  PDF ì²˜ë¦¬"""
    pdf_files = list(Path(directory_path).glob("*.pdf"))
    print(f"ì´ {len(pdf_files)}ê°œì˜ PDF íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

    all_splits = []
    for pdf_file in tqdm.tqdm(pdf_files):
        print(f"\nì²˜ë¦¬ ì¤‘: {pdf_file.name}")
        try:
            splits = process_pdf(pdf_file)
            all_splits.extend(splits)
            print(f"  -> {len(splits)}ê°œì˜ ì²­í¬ ìƒì„±")
        except Exception as e:
            print(f"  -> ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            continue

    print(f"\nì´ {len(all_splits)}ê°œì˜ ì²­í¬ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return all_splits


def main():
    # 1. PDF ì²˜ë¦¬ ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„± (ë˜ëŠ” ê¸°ì¡´ ì €ì¥ì†Œ ë¡œë“œ)
    print("=== ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ===")

    embeddings = AzureOpenAIEmbeddings(
        model=AOAI_DEPLOY_EMBED_3_LARGE,
        openai_api_version="2024-08-01-preview",
        api_key=AOAI_API_KEY,
        azure_endpoint=AOAI_ENDPOINT,
    )

    # ê¸°ì¡´ ì €ì¥ì†Œê°€ ìˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
    try:
        vectorstore = FAISS.load_local(
            folder_path="./faiss_db2",
            embeddings=embeddings,
            allow_dangerous_deserialization=True,
        )
        print("ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    except:
        print("=== PDF ì²˜ë¦¬ ì‹œì‘ ===")
        directory = "./pdf/"
        all_chunks = process_multiple_pdfs(directory)

        print("\n=== ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘ ===")
        vectorstore = FAISS.from_documents(
            documents=all_chunks,
            embedding=embeddings,
        )

        vectorstore.save_local(folder_path="./faiss_db2")


if __name__ == "__main__":
    main()

