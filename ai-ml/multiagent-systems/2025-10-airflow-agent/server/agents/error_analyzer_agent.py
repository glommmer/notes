"""
Error Analyzer Agent - Analyzes errors and searches for solutions
"""

import logging
from typing import Dict, Any
from langchain_openai import ChatOpenAI, AzureChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain.agents import create_openai_tools_agent, AgentExecutor
from server.workflow.state import AgentState, AgentType
from server.services.airflow_client import AirflowClient, AirflowAPIError
from server.tools.search import create_search_tool
from server.config import settings

logger = logging.getLogger(__name__)


class ErrorAnalyzerAgent:
    """
    Agent responsible for analyzing errors and finding solutions

    This agent:
    1. Retrieves task logs from Airflow
    2. Retrieves DAG source code if needed
    3. Uses LLM with tools to analyze the error
    4. Searches internet for solutions
    5. Generates analysis report with root cause and solutions
    """

    def __init__(self, airflow_client: AirflowClient):
        """
        Initialize analyzer agent

        Args:
            airflow_client: Configured Airflow API client
        """
        self.client = airflow_client
        self.agent_type = AgentType.ANALYZER

        try:
            # Initialize LLM
            self.llm = ChatOpenAI(
                model=settings.OPENAI_MODEL,
                temperature=settings.OPENAI_TEMPERATURE,
                api_key=settings.OPENAI_API_KEY,
                base_url=settings.OPENAI_BASE_URL,
            )

            # self.llm = AzureChatOpenAI(
            #     openai_api_key=settings.AZURE_OPENAI_API_KEY,
            #     azure_endpoint=settings.AZURE_OPENAI_ENDPOINT,
            #     azure_deployment=settings.AZURE_OPENAI_DEPLOYMENT,
            #     api_version=settings.AZURE_OPENAI_API_VERSION,
            #     temperature=0.7,
            #     # streaming=True,  # Ïä§Ìä∏Î¶¨Î∞ç ÌôúÏÑ±Ìôî
            # )

            # Setup tools
            self.tools = [create_search_tool()]

            # Create analysis agent
            self.analysis_agent = self._create_analysis_agent()

            logger.info("ErrorAnalyzerAgent initialized successfully")

        except Exception as e:
            logger.error(f"Failed to initialize ErrorAnalyzerAgent: {str(e)}")
            raise

    def _create_analysis_agent(self) -> AgentExecutor:
        """
        Create LangChain agent with tools for analysis

        Returns:
            AgentExecutor instance
        """
        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    """ÎãπÏã†ÏùÄ Apache Airflow Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§.

ÎãπÏã†Ïùò ÏûÑÎ¨¥Îäî Ïã§Ìå®Ìïú Airflow TaskÏùò Î°úÍ∑∏ÏôÄ ÏÜåÏä§ÏΩîÎìúÎ•º Î∂ÑÏÑùÌïòÏó¨:
1. Ïò§Î•òÏùò Í∑ºÎ≥∏ ÏõêÏù∏(root cause)ÏùÑ ÌååÏïÖÌïòÍ≥†
2. Íµ¨Ï≤¥Ï†ÅÏù∏ Ìï¥Í≤∞ Î∞©Î≤ïÏùÑ Ï†úÏãúÌïòÎäî Í≤ÉÏûÖÎãàÎã§.

ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÎèÑÍµ¨:
- internet_search: ÏóêÎü¨ Î©îÏãúÏßÄ, Ïä§ÌÉù Ìä∏Î†àÏù¥Ïä§, ÎùºÏù¥Î∏åÎü¨Î¶¨ Í¥ÄÎ†® Ï†ïÎ≥¥Î•º Í≤ÄÏÉâ

Î∂ÑÏÑù Ïãú Í≥†Î†§ÏÇ¨Ìï≠:
- Python Ïä§ÌÉù Ìä∏Î†àÏù¥Ïä§ÏóêÏÑú Ïã§Ï†ú ÏóêÎü¨ ÏõêÏù∏ ÌååÏïÖ
- ÎùºÏù¥Î∏åÎü¨Î¶¨ Î≤ÑÏ†Ñ Ìò∏ÌôòÏÑ± Î¨∏Ï†ú
- ÏÑ§Ï†ï Ïò§Î•ò (connection, variable Îì±)
- Îç∞Ïù¥ÌÑ∞ ÌíàÏßà Î¨∏Ï†ú
- Î¶¨ÏÜåÏä§ Î∂ÄÏ°± (Î©îÎ™®Î¶¨, ÎîîÏä§ÌÅ¨ Îì±)

Î∞òÎìúÏãú Îã§Ïùå ÌòïÏãùÏúºÎ°ú ÎãµÎ≥ÄÌïòÏÑ∏Ïöî:

## üîç Ïò§Î•ò Î∂ÑÏÑù

### Í∑ºÎ≥∏ ÏõêÏù∏ (Root Cause)
[Ïò§Î•òÏùò Í∑ºÎ≥∏ ÏõêÏù∏ÏùÑ Î™ÖÌôïÌïòÍ≤å ÏÑ§Î™Ö]

### Ïò§Î•ò Î∞úÏÉù ÏúÑÏπò
[Ïñ¥Îäê ÏΩîÎìú/ÎùºÏù∏ÏóêÏÑú Î∞úÏÉùÌñàÎäîÏßÄ]

### Ïò§Î•ò Ïú†Ìòï
[ÏóêÎü¨Ïùò Ïπ¥ÌÖåÍ≥†Î¶¨: Python Exception, Database Error, Network Error Îì±]

## üí° Ìï¥Í≤∞ Î∞©Ïïà

### Í∂åÏû• Ìï¥Í≤∞ Î∞©Î≤ï
1. [Íµ¨Ï≤¥Ï†ÅÏù∏ Ìï¥Í≤∞ Îã®Í≥Ñ]
2. [ÌïÑÏöîÌïú Í≤ΩÏö∞ ÏΩîÎìú ÏàòÏ†ï ÏòàÏãú]
3. [ÏÑ§Ï†ï Î≥ÄÍ≤Ω Î∞©Î≤ï]

### ÏòàÎ∞© Ï°∞Ïπò
- [Ïû¨Î∞ú Î∞©ÏßÄÎ•º ÏúÑÌïú Ï†úÏïà]

## üîß Ï†úÏïà Ïï°ÏÖò
- [ ] Task Ïû¨Ïã§Ìñâ (Clear)
- [ ] ÏΩîÎìú ÏàòÏ†ï ÌïÑÏöî
- [ ] ÏÑ§Ï†ï Î≥ÄÍ≤Ω ÌïÑÏöî
- [ ] Ïù∏ÌîÑÎùº Ï†êÍ≤Ä ÌïÑÏöî
""",
                ),
                ("human", "{input}"),
                ("placeholder", "{agent_scratchpad}"),
            ]
        )

        agent = create_openai_tools_agent(self.llm, self.tools, prompt)

        return AgentExecutor(
            agent=agent,
            tools=self.tools,
            verbose=True,
            max_iterations=3,  # Reduced to prevent timeout
            handle_parsing_errors=True,
            return_intermediate_steps=False,  # Simplify output
        )

    def __call__(self, state: AgentState) -> Dict[str, Any]:
        """
        Execute error analysis

        Args:
            state: Current workflow state

        Returns:
            Updated state with analysis results
        """
        logger.info("üî¨ ErrorAnalyzerAgent: Starting analysis...")

        dag_id = state.get("dag_id")
        dag_run_id = state.get("dag_run_id")
        task_id = state.get("task_id")
        try_number = state.get("try_number", 1)

        if not all([dag_id, dag_run_id, task_id]):
            logger.error("Missing required identifiers")
            return {
                "analysis_report": "Ïò§Î•ò Î∂ÑÏÑù Ïã§Ìå®: ÌïÑÏàò Ï†ïÎ≥¥ ÎàÑÎùΩ (DAG ID, Run ID, Task ID)",
                "current_agent": self.agent_type.value,
            }

        try:
            # Step 1: Get task logs
            logger.info("Retrieving task logs...")
            try:
                logs = self.client.get_task_log(dag_id, dag_run_id, task_id, try_number)
                logger.info(f"Retrieved {len(logs)} characters of logs")
            except AirflowAPIError as e:
                logger.error(f"Failed to get task logs: {e}")
                logs = f"Î°úÍ∑∏Î•º Í∞ÄÏ†∏Ïò¨ Ïàò ÏóÜÏäµÎãàÎã§: {str(e)}"

            # Step 2: Get DAG source code
            logger.info("Retrieving DAG source code...")
            dag_source = None
            try:
                dag_details = self.client.get_dag_details(dag_id)
                file_token = dag_details.get("file_token")
                if file_token:
                    dag_source = self.client.get_dag_source(dag_id)
                    logger.info(
                        f"Retrieved {len(dag_source)} characters of source code"
                    )
                else:
                    dag_source = "ÏÜåÏä§ ÏΩîÎìúÏùò file_tokenÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§."
            except AirflowAPIError as e:
                logger.warning(f"Could not retrieve DAG source: {e}")
                dag_source = f"ÏÜåÏä§ ÏΩîÎìúÎ•º Í∞ÄÏ†∏Ïò¨ Ïàò ÏóÜÏäµÎãàÎã§: {str(e)}"

            # Step 3: Prepare analysis input
            analysis_input = self._prepare_analysis_input(
                dag_id, task_id, logs, dag_source
            )

            # Step 4: Run LLM analysis with tools
            logger.info("Running LLM analysis...")
            try:
                result = self.analysis_agent.invoke({"input": analysis_input})
                analysis_report = result.get("output", "Î∂ÑÏÑù Ïã§Ìå®")
                logger.info("LLM analysis completed successfully")
            except Exception as llm_error:
                logger.error(f"LLM analysis failed: {str(llm_error)}")
                # Fallback: Create basic analysis without LLM
                analysis_report = self._create_fallback_analysis(logs)

            # Extract root cause and solution from report
            root_cause, solution = self._extract_key_info(analysis_report)

            logger.info("Analysis completed successfully")

            return {
                "logs": logs[:2000],  # Truncate for state
                "dag_source": dag_source[:2000] if dag_source else None,  # Truncate
                "analysis_report": analysis_report,
                "root_cause": root_cause,
                "suggested_solution": solution,
                "current_agent": self.agent_type.value,
            }

        except Exception as e:
            logger.error(f"Analysis error: {str(e)}", exc_info=True)
            return {
                "analysis_report": f"Ïò§Î•ò Î∂ÑÏÑù Ï§ë Î¨∏Ï†ú Î∞úÏÉù: {str(e)}",
                "root_cause": "Î∂ÑÏÑù Ïã§Ìå®",
                "suggested_solution": "ÏàòÎèôÏúºÎ°ú Î°úÍ∑∏Î•º ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî",
                "current_agent": self.agent_type.value,
            }

    def _prepare_analysis_input(
        self, dag_id: str, task_id: str, logs: str, dag_source: str
    ) -> str:
        """
        Prepare input for analysis agent

        Args:
            dag_id: DAG identifier
            task_id: Task identifier
            logs: Task execution logs
            dag_source: DAG source code

        Returns:
            Formatted input string
        """
        # Truncate logs and source for LLM context
        log_sample = logs[-3000:] if len(logs) > 3000 else logs
        source_sample = (
            dag_source[:2000] if dag_source and len(dag_source) > 2000 else dag_source
        )

        input_text = f"""Îã§Ïùå Airflow TaskÏùò Ïã§Ìå® ÏõêÏù∏ÏùÑ Î∂ÑÏÑùÌï¥Ï£ºÏÑ∏Ïöî.

## Task Ï†ïÎ≥¥
- DAG ID: {dag_id}
- Task ID: {task_id}

## Ïã§Ìñâ Î°úÍ∑∏ (ÎßàÏßÄÎßâ Î∂ÄÎ∂Ñ)
```
{log_sample}
```

## DAG ÏÜåÏä§ ÏΩîÎìú (ÏùºÎ∂Ä)
```python
{source_sample if source_sample else 'ÏÜåÏä§ ÏΩîÎìúÎ•º ÏÇ¨Ïö©Ìï† Ïàò ÏóÜÏäµÎãàÎã§.'}
```

ÏúÑ Ï†ïÎ≥¥Î•º Î∞îÌÉïÏúºÎ°ú Ïò§Î•òÎ•º Î∂ÑÏÑùÌïòÍ≥†, ÌïÑÏöîÌïú Í≤ΩÏö∞ Ïù∏ÌÑ∞ÎÑ∑ Í≤ÄÏÉâÏùÑ ÌÜµÌï¥ 
Í¥ÄÎ†® Ï†ïÎ≥¥Î•º Ï∞æÏïÑ Ï¢ÖÌï©Ï†ÅÏù∏ Î∂ÑÏÑù Î≥¥Í≥†ÏÑúÎ•º ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.
"""
        return input_text

    def _create_fallback_analysis(self, logs: str) -> str:
        """
        Create fallback analysis when LLM fails

        Args:
            logs: Task logs

        Returns:
            Basic analysis report
        """
        return f"""## üîç Ïò§Î•ò Î∂ÑÏÑù (ÏûêÎèô ÏÉùÏÑ±)

### Î°úÍ∑∏ Ï†ïÎ≥¥
```
{logs[-1000:]}
```

### Í∂åÏû• ÏÇ¨Ìï≠
1. ÏúÑ Î°úÍ∑∏Î•º Í≤ÄÌÜ†ÌïòÏó¨ ÏóêÎü¨ Î©îÏãúÏßÄÎ•º ÌôïÏù∏ÌïòÏÑ∏Ïöî
2. Airflow UIÏóêÏÑú Ï†ÑÏ≤¥ Î°úÍ∑∏Î•º ÌôïÏù∏ÌïòÏÑ∏Ïöî
3. ÏóêÎü¨ Î©îÏãúÏßÄÎ•º Í≤ÄÏÉâÌïòÏó¨ Ìï¥Í≤∞ Î∞©Î≤ïÏùÑ Ï∞æÏïÑÎ≥¥ÏÑ∏Ïöî

### Ï†úÏïà Ïï°ÏÖò
- [ ] Task Ïû¨Ïã§Ìñâ ÏãúÎèÑ
- [ ] ÏàòÎèôÏúºÎ°ú Î°úÍ∑∏ Î∂ÑÏÑù
"""

    def _extract_key_info(self, report: str) -> tuple[str, str]:
        """
        Extract root cause and solution from analysis report

        Args:
            report: Analysis report text

        Returns:
            Tuple of (root_cause, suggested_solution)
        """
        # Simple extraction - look for sections
        root_cause = "Î∂ÑÏÑù Î≥¥Í≥†ÏÑú Ï∞∏Ï°∞"
        solution = "Î∂ÑÏÑù Î≥¥Í≥†ÏÑú Ï∞∏Ï°∞"

        lines = report.split("\n")
        in_root_cause = False
        in_solution = False
        root_lines = []
        solution_lines = []

        for line in lines:
            if "Í∑ºÎ≥∏ ÏõêÏù∏" in line or "Root Cause" in line:
                in_root_cause = True
                in_solution = False
                continue
            elif "Ìï¥Í≤∞ Î∞©Ïïà" in line or "Ìï¥Í≤∞ Î∞©Î≤ï" in line:
                in_solution = True
                in_root_cause = False
                continue
            elif line.startswith("##"):
                in_root_cause = False
                in_solution = False
                continue

            if in_root_cause and line.strip() and not line.startswith("#"):
                root_lines.append(line.strip())
            elif in_solution and line.strip() and not line.startswith("#"):
                solution_lines.append(line.strip())

        if root_lines:
            root_cause = " ".join(root_lines[:2])[:200]  # Truncate
        if solution_lines:
            solution = " ".join(solution_lines[:3])[:300]  # Truncate

        return root_cause, solution
